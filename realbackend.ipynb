{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2GvYWHq99u5"
      },
      "outputs": [],
      "source": [
        "#required packages\n",
        "!pip install flask flask-cors pyngrok newspaper3k lime tensorflow nltk pymupdf lxml==4.9.3 lxml_html_clean requests"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#API for ngrok\n",
        "!ngrok config add-authtoken \"2vMiRmqM75KyG7V5xKNGOelIZ78_EkWAHZkZRzLB4mqRroVt\""
      ],
      "metadata": {
        "id": "Af2n8GJwM3_g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from flask import Flask, request, jsonify\n",
        "from flask_cors import CORS\n",
        "from newspaper import Article\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "from tensorflow.keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import pickle\n",
        "import re\n",
        "import nltk\n",
        "import numpy as np\n",
        "import requests\n",
        "import fitz\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from pyngrok import ngrok\n",
        "import smtplib\n",
        "from email.mime.text import MIMEText\n",
        "\n",
        "#NLTK setup\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "#Flask setup\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "#Preprocessing function\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)\n",
        "    tokens = word_tokenize(text)\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in tokens if w not in stop_words]\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(w) for w in tokens]\n",
        "    return \" \".join(stemmed_tokens)\n",
        "\n",
        "#Load model and tokenizer\n",
        "model = load_model(\"lstm_model.h5\")\n",
        "with open(\"tokenizer.pkl\", \"rb\") as f:\n",
        "    tokenizer = pickle.load(f)\n",
        "\n",
        "MAXLEN = 200\n",
        "\n",
        "#LIME explanation function\n",
        "def explain_with_lime(model, tokenizer, raw_text, maxlen=MAXLEN):\n",
        "    explainer = LimeTextExplainer(class_names=[\"fake\", \"real\"])\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "    def predict_proba(texts):\n",
        "        sequences = tokenizer.texts_to_sequences([preprocess_text(t) for t in texts])\n",
        "        padded = pad_sequences(sequences, maxlen=maxlen)\n",
        "        probs = model.predict(padded, verbose=0)\n",
        "        return np.hstack([1 - probs, probs])\n",
        "\n",
        "    exp = explainer.explain_instance(raw_text, predict_proba, num_features=10)\n",
        "    return [word for word, _ in exp.as_list() if word.lower() not in stop_words]\n",
        "\n",
        "\n",
        "#Extract text from PDF\n",
        "def extract_text_from_pdf(url):\n",
        "    headers = {\n",
        "        \"User-Agent\": (\n",
        "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "            \"Chrome/122.0.0.0 Safari/537.36\"\n",
        "        )\n",
        "    }\n",
        "    response = requests.get(url, headers=headers)\n",
        "    if response.status_code != 200:\n",
        "        raise ValueError(\"Failed to download PDF.\")\n",
        "\n",
        "    with open(\"temp.pdf\", \"wb\") as f:\n",
        "        f.write(response.content)\n",
        "\n",
        "    text = \"\"\n",
        "    doc = fitz.open(\"temp.pdf\")\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "\n",
        "#Main API endpoint\n",
        "@app.route(\"/analyze\", methods=[\"POST\"])\n",
        "def analyze():\n",
        "    try:\n",
        "        url = request.json.get(\"url\")\n",
        "        if not url or not url.startswith(\"http\"):\n",
        "            return jsonify({\"error\": \"Invalid or missing URL\"}), 400\n",
        "\n",
        "        headers = {\n",
        "            \"User-Agent\": (\n",
        "                \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) \"\n",
        "                \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
        "                \"Chrome/122.0.0.0 Safari/537.36\"\n",
        "            )\n",
        "        }\n",
        "\n",
        "        if url.lower().endswith(\".pdf\"):\n",
        "            try:\n",
        "                raw_text = extract_text_from_pdf(url)\n",
        "            except Exception as e:\n",
        "                return jsonify({\"error\": f\"PDF extraction failed: {str(e)}\"}), 400\n",
        "        else:\n",
        "            article = Article(url, request_headers=headers)\n",
        "            try:\n",
        "                article.download()\n",
        "                article.parse()\n",
        "                raw_text = article.text\n",
        "            except Exception as e:\n",
        "                return jsonify({\"error\": f\"Failed to fetch article: {str(e)}\"}), 400\n",
        "\n",
        "        if not raw_text.strip():\n",
        "            return jsonify({\"error\": \"Could not extract article text.\"}), 400\n",
        "\n",
        "        processed = preprocess_text(raw_text)\n",
        "        sequence = tokenizer.texts_to_sequences([processed])\n",
        "        padded = pad_sequences(sequence, maxlen=MAXLEN)\n",
        "        prob = model.predict(padded, verbose=0)[0][0]\n",
        "        credibility = \"High\" if prob < 0.5 else \"Low\"\n",
        "        keywords = explain_with_lime(model, tokenizer, raw_text)\n",
        "\n",
        "        return jsonify({\n",
        "            \"probability\": float(prob),\n",
        "            \"credibility\": credibility,\n",
        "            \"keywords\": keywords\n",
        "        })\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        return jsonify({\"error\": str(e)}), 500\n",
        "\n",
        "#Feedback route sends feedback to my email\n",
        "@app.route(\"/send-feedback\", methods=[\"POST\"])\n",
        "def send_feedback():\n",
        "    data = request.get_json()\n",
        "    feedback = data.get(\"feedback\", \"\")\n",
        "\n",
        "    sender_email = \"ds440capstone@gmail.com\"\n",
        "    receiver_email = \"ds440capstone@gmail.com\"\n",
        "    app_password = \"mvmaqmzwknkwgstb\"\n",
        "\n",
        "    subject = \"New Fake News Detector Feedback\"\n",
        "    body = f\"Feedback submitted:\\n\\n{feedback}\"\n",
        "\n",
        "    msg = MIMEText(body)\n",
        "    msg[\"Subject\"] = subject\n",
        "    msg[\"From\"] = sender_email\n",
        "    msg[\"To\"] = receiver_email\n",
        "\n",
        "    try:\n",
        "        with smtplib.SMTP_SSL(\"smtp.gmail.com\", 465) as server:\n",
        "            server.login(sender_email, app_password)\n",
        "            server.sendmail(sender_email, receiver_email, msg.as_string())\n",
        "        return jsonify({\"message\": \"Feedback sent!\"}), 200\n",
        "    except Exception as e:\n",
        "        print(\"Email failed:\", e)\n",
        "        return jsonify({\"message\": \"Failed to send feedback\"}), 500\n",
        "\n",
        "#Start server with ngrok\n",
        "public_url = ngrok.connect(5000)\n",
        "print(f\"Your public API URL: {public_url}/analyze\")\n",
        "app.run(port=5000)\n"
      ],
      "metadata": {
        "id": "eN8MKLgepByx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}