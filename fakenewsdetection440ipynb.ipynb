{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPQC-drc2vbq",
        "outputId": "fb253aa8-0489-4ffa-cfa6-2b9a641912bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mNqnViCw2Nwj",
        "outputId": "f89ce87f-942e-48c1-d824-2ee51f751bd7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting LIAR dataset processing...\n",
            "Successfully loaded dataset: 10240 training, 1267 test, 1284 validation examples\n",
            "Processed 500/12791 statements\n",
            "Processed 1000/12791 statements\n",
            "Processed 1500/12791 statements\n",
            "Processed 2000/12791 statements\n",
            "Processed 2500/12791 statements\n",
            "Processed 3000/12791 statements\n",
            "Processed 3500/12791 statements\n",
            "Processed 4000/12791 statements\n",
            "Processed 4500/12791 statements\n",
            "Processed 5000/12791 statements\n",
            "Processed 5500/12791 statements\n",
            "Processed 6000/12791 statements\n",
            "Processed 6500/12791 statements\n",
            "Processed 7000/12791 statements\n",
            "Processed 7500/12791 statements\n",
            "Processed 8000/12791 statements\n",
            "Processed 8500/12791 statements\n",
            "Processed 9000/12791 statements\n",
            "Processed 9500/12791 statements\n",
            "Processed 10000/12791 statements\n",
            "Processed 10500/12791 statements\n",
            "Processed 11000/12791 statements\n",
            "Processed 11500/12791 statements\n",
            "Processed 12000/12791 statements\n",
            "Processed 12500/12791 statements\n",
            "Processed dataset saved to 'liar_processed.csv' with 12791 records\n",
            "\n",
            "Dataset Information:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 12791 entries, 0 to 12790\n",
            "Data columns (total 13 columns):\n",
            " #   Column             Non-Null Count  Dtype  \n",
            "---  ------             --------------  -----  \n",
            " 0   id                 12791 non-null  object \n",
            " 1   label              12791 non-null  object \n",
            " 2   statement          12791 non-null  object \n",
            " 3   preprocessed_text  12791 non-null  object \n",
            " 4   num_tokens         12791 non-null  int64  \n",
            " 5   speaker            12789 non-null  object \n",
            " 6   context            12660 non-null  object \n",
            " 7   polarity           12791 non-null  float64\n",
            " 8   subjectivity       12791 non-null  float64\n",
            " 9   compound           12791 non-null  float64\n",
            " 10  pos                12791 non-null  float64\n",
            " 11  neu                12791 non-null  float64\n",
            " 12  neg                12791 non-null  float64\n",
            "dtypes: float64(6), int64(1), object(6)\n",
            "memory usage: 1.3+ MB\n",
            "None\n",
            "\n",
            "Label Distribution:\n",
            "label\n",
            "half-true      2627\n",
            "false          2507\n",
            "mostly-true    2454\n",
            "barely-true    2103\n",
            "true           2053\n",
            "pants-fire     1047\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Sentiment Analysis by Label:\n",
            "             polarity  subjectivity  compound       pos       neu       neg\n",
            "label                                                                      \n",
            "barely-true  0.024615      0.220798  0.011282  0.113736  0.780006  0.106260\n",
            "false        0.018996      0.205702  0.010952  0.108728  0.791474  0.099803\n",
            "half-true    0.029122      0.225853 -0.019980  0.103729  0.780986  0.115281\n",
            "mostly-true  0.015198      0.223671 -0.016521  0.100241  0.790094  0.109658\n",
            "pants-fire   0.018162      0.216312 -0.007550  0.110995  0.773589  0.115417\n",
            "true         0.027252      0.222234 -0.001962  0.102738  0.793600  0.103664\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from textblob import TextBlob\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('vader_lexicon')\n",
        "\n",
        "def download_liar_dataset():\n",
        "    \"\"\"\n",
        "    Function to download the LIAR dataset.\n",
        "\n",
        "    Note: The LIAR dataset needs to be downloaded manually from:\n",
        "    https://www.cs.ucsb.edu/~william/data/liar_dataset.zip\n",
        "\n",
        "    After downloading, unzip the file and place the train, test and valid files\n",
        "    in a directory called 'liar_dataset'.\n",
        "\n",
        "    Returns:\n",
        "        train_df, test_df, valid_df (tuple of pandas DataFrames)\n",
        "    \"\"\"\n",
        "    # Define column names for the datasets\n",
        "    column_names = [\n",
        "        'id', 'label', 'statement', 'subject', 'speaker', 'speaker_job',\n",
        "        'state_info', 'party', 'barely_true_counts', 'false_counts',\n",
        "        'half_true_counts', 'mostly_true_counts', 'pants_on_fire_counts', 'context'\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        # Load datasets\n",
        "        train_df = pd.read_csv('liar_dataset/train.tsv', sep='\\t', header=None, names=column_names)\n",
        "        test_df = pd.read_csv('liar_dataset/test.tsv', sep='\\t', header=None, names=column_names)\n",
        "        valid_df = pd.read_csv('liar_dataset/valid.tsv', sep='\\t', header=None, names=column_names)\n",
        "\n",
        "        print(f\"Successfully loaded dataset: {len(train_df)} training, {len(test_df)} test, {len(valid_df)} validation examples\")\n",
        "        return train_df, test_df, valid_df\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(\"Dataset files not found. Please download the LIAR dataset from:\")\n",
        "        print(\"https://www.cs.ucsb.edu/~william/data/liar_dataset.zip\")\n",
        "        print(\"After downloading, unzip and place the files in a 'liar_dataset' directory.\")\n",
        "        return None, None, None\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Function to preprocess text data.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text to preprocess\n",
        "\n",
        "    Returns:\n",
        "        str: Preprocessed text\n",
        "        list: Tokens\n",
        "        list: Stemmed tokens\n",
        "        list: Lemmatized tokens\n",
        "    \"\"\"\n",
        "    #checks if text is nan\n",
        "    if pd.isna(text):\n",
        "        return \"\", [], [], []\n",
        "\n",
        "    text = text.lower()\n",
        "\n",
        "    #removes URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "\n",
        "    #removes HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "\n",
        "    #removes special characters/numbers\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "    #tokenize\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    #removes stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    #stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
        "\n",
        "    #lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    #reconstruct the preprocessed text\n",
        "    preprocessed_text = ' '.join(tokens)\n",
        "\n",
        "    return preprocessed_text, tokens, stemmed_tokens, lemmatized_tokens\n",
        "\n",
        "def extract_sentiment_features(text):\n",
        "    \"\"\"\n",
        "    Function to extract sentiment features from text.\n",
        "\n",
        "    Args:\n",
        "        text (str): Input text\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary containing sentiment features\n",
        "    \"\"\"\n",
        "    #check if text is empty\n",
        "    if not text:\n",
        "        return {\n",
        "            'polarity': 0,\n",
        "            'subjectivity': 0,\n",
        "            'compound': 0,\n",
        "            'pos': 0,\n",
        "            'neu': 0,\n",
        "            'neg': 0\n",
        "        }\n",
        "\n",
        "    #sentiment analysis (textblob)\n",
        "    blob = TextBlob(text)\n",
        "    polarity = blob.sentiment.polarity\n",
        "    subjectivity = blob.sentiment.subjectivity\n",
        "\n",
        "    #sentiment analysis (VADER)\n",
        "    sid = SentimentIntensityAnalyzer()\n",
        "    vader_scores = sid.polarity_scores(text)\n",
        "\n",
        "    return {\n",
        "        'polarity': polarity,\n",
        "        'subjectivity': subjectivity,\n",
        "        'compound': vader_scores['compound'],\n",
        "        'pos': vader_scores['pos'],\n",
        "        'neu': vader_scores['neu'],\n",
        "        'neg': vader_scores['neg']\n",
        "    }\n",
        "\n",
        "def process_liar_dataset():\n",
        "    \"\"\"\n",
        "    Main function to process the LIAR dataset.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Processed dataset with features\n",
        "    \"\"\"\n",
        "    #download and load the dataset\n",
        "    train_df, test_df, valid_df = download_liar_dataset()\n",
        "\n",
        "    if train_df is None:\n",
        "        return None\n",
        "\n",
        "    #combines datasets for processing\n",
        "    combined_df = pd.concat([train_df, test_df, valid_df], ignore_index=True)\n",
        "\n",
        "    #list to store processed data\n",
        "    processed_data = []\n",
        "\n",
        "    #process each statement\n",
        "    for idx, row in combined_df.iterrows():\n",
        "        statement = row['statement']\n",
        "\n",
        "        #preprocess text\n",
        "        preprocessed_text, tokens, stemmed_tokens, lemmatized_tokens = preprocess_text(statement)\n",
        "\n",
        "        #extracting sentiment features\n",
        "        sentiment_features = extract_sentiment_features(preprocessed_text)\n",
        "\n",
        "        #stores processed data\n",
        "        processed_item = {\n",
        "            'id': row['id'],\n",
        "            'label': row['label'],\n",
        "            'statement': statement,\n",
        "            'preprocessed_text': preprocessed_text,\n",
        "            'num_tokens': len(tokens),\n",
        "            'speaker': row['speaker'],\n",
        "            'context': row['context'],\n",
        "            **sentiment_features  # Unpack sentiment features\n",
        "        }\n",
        "\n",
        "        processed_data.append(processed_item)\n",
        "\n",
        "        #printing the progress every 500 items\n",
        "        if (idx + 1) % 500 == 0:\n",
        "            print(f\"Processed {idx + 1}/{len(combined_df)} statements\")\n",
        "\n",
        "    #creating DataFrame from processed data\n",
        "    processed_df = pd.DataFrame(processed_data)\n",
        "\n",
        "    #saving processed data\n",
        "    processed_df.to_csv('liar_processed.csv', index=False)\n",
        "    print(f\"Processed dataset saved to 'liar_processed.csv' with {len(processed_df)} records\")\n",
        "\n",
        "    return processed_df\n",
        "\n",
        "def analyze_sentiment_distribution(df):\n",
        "    \"\"\"\n",
        "    Analyze sentiment distribution by label.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): Processed dataset\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Sentiment statistics by label\n",
        "    \"\"\"\n",
        "    #group by label/calculate mean sentiment scores\n",
        "    sentiment_by_label = df.groupby('label')[['polarity', 'subjectivity', 'compound', 'pos', 'neu', 'neg']].mean()\n",
        "\n",
        "    print(\"\\nSentiment Analysis by Label:\")\n",
        "    print(sentiment_by_label)\n",
        "\n",
        "    return sentiment_by_label\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"Starting LIAR dataset processing...\")\n",
        "    processed_df = process_liar_dataset()\n",
        "\n",
        "    if processed_df is not None:\n",
        "        #print dataset info\n",
        "        print(\"\\nDataset Information:\")\n",
        "        print(processed_df.info())\n",
        "\n",
        "        #print label distribution\n",
        "        print(\"\\nLabel Distribution:\")\n",
        "        print(processed_df['label'].value_counts())\n",
        "\n",
        "        #analyze sentiment distribution\n",
        "        sentiment_stats = analyze_sentiment_distribution(processed_df)"
      ]
    }
  ]
}